{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2023, Tri Dao.\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import gradcheck\n",
    "import pytest\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, mamba_inner_ref\n",
    "from mamba_ssm.ops.selective_scan_interface import bimamba_inner_fn, bimamba_inner_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pytest.mark.parametrize('wtype', [torch.float32, torch.complex64])\n",
    "@pytest.mark.parametrize('wtype', [torch.float32])\n",
    "# @pytest.mark.parametrize('itype', [torch.float32, torch.float16, torch.bfloat16])\n",
    "@pytest.mark.parametrize('itype', [torch.float32])\n",
    "# @pytest.mark.parametrize('seqlen', [8, 16, 32, 64, 128, 256, 372, 512, 784, 1024, 1134, 2048, 4096])\n",
    "@pytest.mark.parametrize('seqlen', [128, 256, 512, 1024, 2048, 4096])\n",
    "# @pytest.mark.parametrize('seqlen', [128])\n",
    "# @pytest.mark.parametrize(\"return_last_state\", [False, True])\n",
    "@pytest.mark.parametrize(\"return_last_state\", [True])\n",
    "# @pytest.mark.parametrize('has_delta_bias', [False, True])\n",
    "@pytest.mark.parametrize('has_delta_bias', [True])\n",
    "# @pytest.mark.parametrize('delta_softplus', [False, True])\n",
    "@pytest.mark.parametrize('delta_softplus', [True])\n",
    "# @pytest.mark.parametrize('has_z', [False, True])\n",
    "@pytest.mark.parametrize('has_z', [True])\n",
    "# @pytest.mark.parametrize('has_D', [False, True])\n",
    "@pytest.mark.parametrize('has_D', [True])\n",
    "@pytest.mark.parametrize(\"varBC_groups\", [1, 2])\n",
    "# @pytest.mark.parametrize(\"varBC_groups\", [1])\n",
    "# @pytest.mark.parametrize(\"is_variable_C\", [False, True])\n",
    "@pytest.mark.parametrize(\"is_variable_C\", [True])\n",
    "# @pytest.mark.parametrize(\"is_variable_B\", [False, True])\n",
    "@pytest.mark.parametrize(\"is_variable_B\", [True])\n",
    "def test_selective_scan(is_variable_B, is_variable_C, varBC_groups, has_D, has_z, has_delta_bias,\n",
    "                        delta_softplus, return_last_state, seqlen, itype, wtype):\n",
    "    if varBC_groups > 1 and (not is_variable_B or not is_variable_C):\n",
    "        pytest.skip()  # This config is not applicable\n",
    "    device = 'cuda'\n",
    "    rtol, atol = (6e-4, 2e-3) if itype == torch.float32 else (3e-3, 5e-3)\n",
    "    if itype == torch.bfloat16:\n",
    "        rtol, atol = 3e-2, 5e-2\n",
    "    rtolw, atolw = (1e-3, 1e-3)\n",
    "    if has_z:  # If we have z, the errors on the weights seem higher\n",
    "        rtolw = max(rtolw, rtol)\n",
    "        atolw = max(atolw, atol)\n",
    "    # set seed\n",
    "    torch.random.manual_seed(0)\n",
    "    batch_size = 2\n",
    "    dim = 4\n",
    "    dstate = 8\n",
    "    is_complex = wtype == torch.complex64\n",
    "    A = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)).requires_grad_()\n",
    "    if not is_variable_B:\n",
    "        B_shape = (dim, dstate)\n",
    "    elif varBC_groups == 1:\n",
    "        B_shape = (batch_size, dstate, seqlen if not is_complex else seqlen * 2)\n",
    "    else:\n",
    "        B_shape = (batch_size, varBC_groups, dstate, seqlen if not is_complex else seqlen * 2)\n",
    "    B = torch.randn(*B_shape, device=device, dtype=wtype if not is_variable_B else itype,\n",
    "                    requires_grad=True)\n",
    "    if not is_variable_C:\n",
    "        C_shape = (dim, dstate)\n",
    "    elif varBC_groups == 1:\n",
    "        C_shape = (batch_size, dstate, seqlen if not is_complex else seqlen * 2)\n",
    "    else:\n",
    "        C_shape = (batch_size, varBC_groups, dstate, seqlen if not is_complex else seqlen * 2)\n",
    "    C = torch.randn(*C_shape, device=device, dtype=wtype if not is_variable_C else itype,\n",
    "                    requires_grad=True)\n",
    "    if has_D:\n",
    "        D = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    else:\n",
    "        D = None\n",
    "    if has_z:\n",
    "        z = torch.randn(batch_size, dim, seqlen, device=device, dtype=itype, requires_grad=True)\n",
    "    else:\n",
    "        z = None\n",
    "    if has_delta_bias:\n",
    "        delta_bias = (0.5 * torch.rand(dim, device=device, dtype=torch.float32)).requires_grad_()\n",
    "    else:\n",
    "        delta_bias = None\n",
    "    u = torch.randn(batch_size, dim, seqlen, device=device, dtype=itype, requires_grad=True)\n",
    "    delta = (0.5 * torch.rand(batch_size, dim, seqlen, device=device, dtype=itype)).requires_grad_()\n",
    "    A_ref = A.detach().clone().requires_grad_()\n",
    "    B_ref = B.detach().clone().requires_grad_()\n",
    "    C_ref = C.detach().clone().requires_grad_()\n",
    "    D_ref = D.detach().clone().requires_grad_() if D is not None else None\n",
    "    z_ref = z.detach().clone().requires_grad_() if z is not None else None\n",
    "    u_ref = u.detach().clone().requires_grad_()\n",
    "    delta_ref = delta.detach().clone().requires_grad_()\n",
    "    delta_bias_ref = delta_bias.detach().clone().requires_grad_() if delta_bias is not None else None\n",
    "    out, *rest = selective_scan_fn(\n",
    "        u, delta, A, B, C, D, z=z,\n",
    "        delta_bias=delta_bias, delta_softplus=delta_softplus,\n",
    "        return_last_state=return_last_state\n",
    "    )\n",
    "    if return_last_state:\n",
    "        state = rest[0]\n",
    "    out_ref, *rest = selective_scan_ref(\n",
    "        u_ref, delta_ref, A_ref, B_ref, C_ref, D_ref, z=z_ref,\n",
    "        delta_bias=delta_bias_ref, delta_softplus=delta_softplus,\n",
    "        return_last_state=return_last_state\n",
    "    )\n",
    "    if return_last_state:\n",
    "        state_ref = rest[0]\n",
    "    # dA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "    # dt_u = delta * u\n",
    "\n",
    "    print(f'Output max diff: {(out - out_ref).abs().max().item()}')\n",
    "    print(f'Output mean diff: {(out - out_ref).abs().mean().item()}')\n",
    "    assert torch.allclose(out, out_ref, rtol=rtol, atol=atol)\n",
    "    if return_last_state:\n",
    "        print(f'State max diff: {(state - state_ref).abs().max().item()}')\n",
    "        assert torch.allclose(state, state_ref, rtol=rtol, atol=atol)\n",
    "\n",
    "    g = torch.randn_like(out)\n",
    "    out_ref.backward(g)\n",
    "    out.backward(g)\n",
    "\n",
    "    print(f'du max diff: {(u.grad - u_ref.grad).abs().max().item()}')\n",
    "    print(f'ddelta max diff: {(delta.grad - delta_ref.grad).abs().max().item()}')\n",
    "    print(f'dA max diff: {(A.grad - A_ref.grad).abs().max().item()}')\n",
    "    print(f'dB max diff: {(B.grad - B_ref.grad).abs().max().item()}')\n",
    "    print(f'dC max diff: {(C.grad - C_ref.grad).abs().max().item()}')\n",
    "    if has_D:\n",
    "        print(f'dD max diff: {(D.grad - D_ref.grad).abs().max().item()}')\n",
    "    if has_z:\n",
    "        print(f'dz max diff: {(z.grad - z_ref.grad).abs().max().item()}')\n",
    "    if has_delta_bias:\n",
    "        print(f'ddelta_bias max diff: {(delta_bias.grad - delta_bias_ref.grad).abs().max().item()}')\n",
    "\n",
    "    assert torch.allclose(u.grad, u_ref.grad.to(dtype=itype), rtol=rtol * 2, atol=atol * 2)\n",
    "    assert torch.allclose(delta.grad, delta_ref.grad.to(dtype=itype), rtol=rtol * 5, atol=atol * 10)\n",
    "    assert torch.allclose(A.grad, A_ref.grad, rtol=rtolw, atol=atolw * 5)\n",
    "    assert torch.allclose(B.grad, B_ref.grad, rtol=rtolw if not is_variable_B else rtol,\n",
    "                          atol=atolw if not is_variable_B else atol)\n",
    "    assert torch.allclose(C.grad, C_ref.grad, rtol=rtolw if not is_variable_C else rtol,\n",
    "                          atol=atolw if not is_variable_C else atol)\n",
    "    if has_D:\n",
    "        assert torch.allclose(D.grad, D_ref.grad, rtol=rtolw, atol=atolw)\n",
    "    if has_z:\n",
    "        assert torch.allclose(z.grad, z_ref.grad, rtol=rtolw, atol=atolw)\n",
    "    if has_delta_bias:\n",
    "        assert torch.allclose(delta_bias.grad, delta_bias_ref.grad, rtol=rtolw, atol=atolw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize('wtype', [torch.float32, torch.complex64])\n",
    "# @pytest.mark.parametrize('wtype', [torch.complex64])\n",
    "# @pytest.mark.parametrize('itype', [torch.float32, torch.float16, torch.bfloat16])\n",
    "@pytest.mark.parametrize('itype', [torch.float32])\n",
    "# @pytest.mark.parametrize('seqlen', [8, 16, 32, 64, 128, 256, 372, 512, 784, 1024, 1134, 2048, 4096])\n",
    "@pytest.mark.parametrize('seqlen', [128])\n",
    "@pytest.mark.parametrize(\"is_variable_C\", [False, True])\n",
    "# @pytest.mark.parametrize(\"is_variable_C\", [False])\n",
    "@pytest.mark.parametrize(\"is_variable_B\", [False, True])\n",
    "# @pytest.mark.parametrize(\"is_variable_B\", [True])\n",
    "def test_mamba_inner_fn(is_variable_B, is_variable_C, seqlen, itype, wtype):\n",
    "    device = 'cuda'\n",
    "    rtol, atol = (6e-4, 2e-3) if itype == torch.float32 else (3e-3, 5e-3)\n",
    "    if itype == torch.bfloat16:\n",
    "        rtol, atol = 3e-2, 5e-2\n",
    "    rtolw, atolw = (1e-3, 1e-3)\n",
    "    # If we have z, the errors on the weights seem higher\n",
    "    rtolw = max(rtolw, rtol)\n",
    "    atolw = max(atolw, atol)\n",
    "    # set seed\n",
    "    torch.random.manual_seed(0)\n",
    "    batch_size = 2\n",
    "    dim = 768\n",
    "    dstate = 8\n",
    "    dt_rank = 48\n",
    "    is_complex = wtype == torch.complex64\n",
    "    xz = torch.randn(batch_size, 2 * dim, seqlen, device=device, dtype=itype, requires_grad=True)\n",
    "    conv1d_weight = torch.randn(dim, 1, 3, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    conv1d_bias = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    x_proj_weight = torch.randn(dt_rank + (bool(is_variable_B) + bool(is_variable_C)) * dstate\n",
    "                                * (1 if not is_complex else 2),\n",
    "                                dim, device=device, dtype=itype, requires_grad=True)\n",
    "    delta_proj_weight = torch.randn(dim, dt_rank, device=device, dtype=itype, requires_grad=True)\n",
    "    out_proj_weight = torch.randn(dim // 2, dim, device=device, dtype=itype, requires_grad=True)\n",
    "    out_proj_bias = None\n",
    "    A = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)).requires_grad_()\n",
    "    B = (torch.randn(dim, dstate, device=device, dtype=wtype, requires_grad=True)\n",
    "         if not is_variable_B else None)\n",
    "    C = (torch.randn(dim, dstate, device=device, dtype=wtype, requires_grad=True)\n",
    "         if not is_variable_C else None)\n",
    "    D = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    delta_bias = (0.5 * torch.rand(dim, device=device, dtype=torch.float32)).requires_grad_()\n",
    "    B_proj_bias = None\n",
    "    C_proj_bias = None\n",
    "    xz_ref = xz.detach().clone().requires_grad_()\n",
    "    conv1d_weight_ref = conv1d_weight.detach().clone().requires_grad_()\n",
    "    conv1d_bias_ref = conv1d_bias.detach().clone().requires_grad_()\n",
    "    x_proj_weight_ref = x_proj_weight.detach().clone().requires_grad_()\n",
    "    delta_proj_weight_ref = delta_proj_weight.detach().clone().requires_grad_()\n",
    "    out_proj_weight_ref = out_proj_weight.detach().clone().requires_grad_()\n",
    "    out_proj_bias_ref = (out_proj_bias.detach().clone().requires_grad_()\n",
    "                         if out_proj_bias is not None else None)\n",
    "    A_ref = A.detach().clone().requires_grad_()\n",
    "    B_ref = B.detach().clone().requires_grad_() if B is not None else None\n",
    "    C_ref = C.detach().clone().requires_grad_() if C is not None else None\n",
    "    D_ref = D.detach().clone().requires_grad_()\n",
    "    delta_bias_ref = delta_bias.detach().clone().requires_grad_() if delta_bias is not None else None\n",
    "    out = mamba_inner_fn(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                         out_proj_weight, out_proj_bias,\n",
    "                         A, B, C, D, delta_bias=delta_bias, delta_softplus=True)\n",
    "    out_ref = mamba_inner_ref(xz_ref, conv1d_weight_ref, conv1d_bias_ref, x_proj_weight_ref,\n",
    "                              delta_proj_weight_ref, out_proj_weight_ref, out_proj_bias_ref,\n",
    "                              A_ref, B_ref, C_ref, D_ref,\n",
    "                              delta_bias=delta_bias_ref, delta_softplus=True)\n",
    "    # dA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "    # dt_u = delta * u\n",
    "    print(\"mamba_inner_fn\")\n",
    "    print(f'Output max diff: {(out - out_ref).abs().max().item()}')\n",
    "    print(f'Output mean diff: {(out - out_ref).abs().mean().item()}')\n",
    "    assert torch.allclose(out, out_ref, rtol=rtol, atol=atol)\n",
    "\n",
    "    g = torch.randn_like(out)\n",
    "    out_ref.backward(g)\n",
    "    out.backward(g)\n",
    "\n",
    "    print(f'dxz max diff: {(xz.grad - xz_ref.grad).abs().max().item()}')\n",
    "    print(f'dA max diff: {(A.grad - A_ref.grad).abs().max().item()}')\n",
    "    if not is_variable_B:\n",
    "        print(f'dB max diff: {(B.grad - B_ref.grad).abs().max().item()}')\n",
    "    if not is_variable_C:\n",
    "        print(f'dC max diff: {(C.grad - C_ref.grad).abs().max().item()}')\n",
    "    print(f'dD max diff: {(D.grad - D_ref.grad).abs().max().item()}')\n",
    "    print(f'ddelta_bias max diff: {(delta_bias.grad - delta_bias_ref.grad).abs().max().item()}')\n",
    "    print(f'dout_proj_weight max diff: {(out_proj_weight.grad - out_proj_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'ddelta_proj_weight max diff: {(delta_proj_weight.grad - delta_proj_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'dx_proj_weight max diff: {(x_proj_weight.grad - x_proj_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'dconv1d_weight max diff: {(conv1d_weight.grad - conv1d_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'dconv1d_bias max diff: {(conv1d_bias.grad - conv1d_bias_ref.grad).abs().max().item()}')\n",
    "\n",
    "    # assert torch.allclose(xz.grad, xz_ref.grad.to(dtype=itype), rtol=rtol * 2, atol=atol * 2)\n",
    "    # assert torch.allclose(delta.grad, delta_ref.grad.to(dtype=itype), rtol=rtol * 5, atol=atol * 10)\n",
    "    # assert torch.allclose(A.grad, A_ref.grad, rtol=rtolw, atol=atolw * 5)\n",
    "    # assert torch.allclose(B.grad, B_ref.grad, rtol=rtolw if not is_variable_B else rtol,\n",
    "    #                       atol=atolw if not is_variable_B else atol)\n",
    "    # assert torch.allclose(C.grad, C_ref.grad, rtol=rtolw if not is_variable_C else rtol,\n",
    "    #                       atol=atolw if not is_variable_C else atol)\n",
    "    # assert torch.allclose(D.grad, D_ref.grad, rtol=rtolw, atol=atolw)\n",
    "    # assert torch.allclose(delta_bias.grad, delta_bias_ref.grad, rtol=rtolw, atol=atolw)\n",
    "\n",
    "\n",
    "# test_mamba_inner_fn(False, False, 128, torch.float32, torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize('wtype', [torch.float32, torch.complex64])\n",
    "# @pytest.mark.parametrize('wtype', [torch.complex64])\n",
    "# @pytest.mark.parametrize('itype', [torch.float32, torch.float16, torch.bfloat16])\n",
    "@pytest.mark.parametrize('itype', [torch.float32])\n",
    "# @pytest.mark.parametrize('seqlen', [8, 16, 32, 64, 128, 256, 372, 512, 784, 1024, 1134, 2048, 4096])\n",
    "@pytest.mark.parametrize('seqlen', [128])\n",
    "@pytest.mark.parametrize(\"is_variable_C\", [False, True])\n",
    "# @pytest.mark.parametrize(\"is_variable_C\", [False])\n",
    "@pytest.mark.parametrize(\"is_variable_B\", [False, True])\n",
    "# @pytest.mark.parametrize(\"is_variable_B\", [True])\n",
    "def test_bimamba_inner_fn(is_variable_B, is_variable_C, seqlen, itype, wtype):\n",
    "    device = 'cuda'\n",
    "    rtol, atol = (6e-4, 2e-3) if itype == torch.float32 else (3e-3, 5e-3)\n",
    "    if itype == torch.bfloat16:\n",
    "        rtol, atol = 3e-2, 5e-2\n",
    "    rtolw, atolw = (1e-3, 1e-3)\n",
    "    # If we have z, the errors on the weights seem higher\n",
    "    rtolw = max(rtolw, rtol)\n",
    "    atolw = max(atolw, atol)\n",
    "    # set seed\n",
    "    torch.random.manual_seed(0)\n",
    "    batch_size = 2\n",
    "    dim = 768\n",
    "    dstate = 8\n",
    "    dt_rank = 48\n",
    "    is_complex = wtype == torch.complex64\n",
    "    xz = torch.randn(batch_size, 2 * dim, seqlen, device=device, dtype=itype, requires_grad=True)\n",
    "    conv1d_weight = torch.randn(dim, 1, 3, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    conv1d_bias = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    x_proj_weight = torch.randn(dt_rank + (bool(is_variable_B) + bool(is_variable_C)) * dstate\n",
    "                                * (1 if not is_complex else 2),\n",
    "                                dim, device=device, dtype=itype, requires_grad=True)\n",
    "    delta_proj_weight = torch.randn(dim, dt_rank, device=device, dtype=itype, requires_grad=True)\n",
    "    out_proj_weight = torch.randn(dim // 2, dim, device=device, dtype=itype, requires_grad=True)\n",
    "    out_proj_bias = None\n",
    "    A = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)).requires_grad_()\n",
    "    A_b = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)).requires_grad_()\n",
    "    B = (torch.randn(dim, dstate, device=device, dtype=wtype, requires_grad=True)\n",
    "         if not is_variable_B else None)\n",
    "    C = (torch.randn(dim, dstate, device=device, dtype=wtype, requires_grad=True)\n",
    "         if not is_variable_C else None)\n",
    "    D = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    delta_bias = (0.5 * torch.rand(dim, device=device, dtype=torch.float32)).requires_grad_()\n",
    "    B_proj_bias = None\n",
    "    C_proj_bias = None\n",
    "    xz_ref = xz.detach().clone().requires_grad_()\n",
    "    conv1d_weight_ref = conv1d_weight.detach().clone().requires_grad_()\n",
    "    conv1d_bias_ref = conv1d_bias.detach().clone().requires_grad_()\n",
    "    x_proj_weight_ref = x_proj_weight.detach().clone().requires_grad_()\n",
    "    delta_proj_weight_ref = delta_proj_weight.detach().clone().requires_grad_()\n",
    "    out_proj_weight_ref = out_proj_weight.detach().clone().requires_grad_()\n",
    "    out_proj_bias_ref = (out_proj_bias.detach().clone().requires_grad_()\n",
    "                         if out_proj_bias is not None else None)\n",
    "    A_ref = A.detach().clone().requires_grad_()\n",
    "    A_b_ref = A_b.detach().clone().requires_grad_()\n",
    "    B_ref = B.detach().clone().requires_grad_() if B is not None else None\n",
    "    C_ref = C.detach().clone().requires_grad_() if C is not None else None\n",
    "    D_ref = D.detach().clone().requires_grad_()\n",
    "    delta_bias_ref = delta_bias.detach().clone().requires_grad_() if delta_bias is not None else None\n",
    "    out = bimamba_inner_fn(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                         out_proj_weight, out_proj_bias,\n",
    "                         A, A_b, B, C, D, delta_bias=delta_bias, delta_softplus=True)\n",
    "    out_ref = bimamba_inner_fn(xz_ref, conv1d_weight_ref, conv1d_bias_ref, x_proj_weight_ref,\n",
    "                              delta_proj_weight_ref, out_proj_weight_ref, out_proj_bias_ref,\n",
    "                              A_ref, A_b_ref, B_ref, C_ref, D_ref,\n",
    "                              delta_bias=delta_bias_ref, delta_softplus=True)\n",
    "    # dA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "    # dt_u = delta * u\n",
    "    print(\"bimamba_inner_fn\")\n",
    "    print(f'Output max diff: {(out - out_ref).abs().max().item()}')\n",
    "    print(f'Output mean diff: {(out - out_ref).abs().mean().item()}')\n",
    "    assert torch.allclose(out, out_ref, rtol=rtol, atol=atol)\n",
    "\n",
    "    g = torch.randn_like(out)\n",
    "    out_ref.backward(g)\n",
    "    out.backward(g)\n",
    "\n",
    "    print(f'dxz max diff: {(xz.grad - xz_ref.grad).abs().max().item()}')\n",
    "    print(f'dA max diff: {(A.grad - A_ref.grad).abs().max().item()}')\n",
    "    print(f'dA_b max diff: {(A_b.grad - A_b_ref.grad).abs().max().item()}')\n",
    "    if not is_variable_B:\n",
    "        print(f'dB max diff: {(B.grad - B_ref.grad).abs().max().item()}')\n",
    "    if not is_variable_C:\n",
    "        print(f'dC max diff: {(C.grad - C_ref.grad).abs().max().item()}')\n",
    "    print(f'dD max diff: {(D.grad - D_ref.grad).abs().max().item()}')\n",
    "    print(f'ddelta_bias max diff: {(delta_bias.grad - delta_bias_ref.grad).abs().max().item()}')\n",
    "    print(f'dout_proj_weight max diff: {(out_proj_weight.grad - out_proj_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'ddelta_proj_weight max diff: {(delta_proj_weight.grad - delta_proj_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'dx_proj_weight max diff: {(x_proj_weight.grad - x_proj_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'dconv1d_weight max diff: {(conv1d_weight.grad - conv1d_weight_ref.grad).abs().max().item()}')\n",
    "    print(f'dconv1d_bias max diff: {(conv1d_bias.grad - conv1d_bias_ref.grad).abs().max().item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize('wtype', [torch.float32, torch.complex64])\n",
    "# @pytest.mark.parametrize('wtype', [torch.complex64])\n",
    "# @pytest.mark.parametrize('itype', [torch.float32, torch.float16, torch.bfloat16])\n",
    "@pytest.mark.parametrize('itype', [torch.float32])\n",
    "# @pytest.mark.parametrize('seqlen', [8, 16, 32, 64, 128, 256, 372, 512, 784, 1024, 1134, 2048, 4096])\n",
    "@pytest.mark.parametrize('seqlen', [128])\n",
    "@pytest.mark.parametrize(\"is_variable_C\", [False, True])\n",
    "# @pytest.mark.parametrize(\"is_variable_C\", [False])\n",
    "@pytest.mark.parametrize(\"is_variable_B\", [False, True])\n",
    "# @pytest.mark.parametrize(\"is_variable_B\", [True])\n",
    "def test_bimamba_inner_fn_grad_check(is_variable_B, is_variable_C, seqlen, itype, wtype):\n",
    "    device = 'cuda'\n",
    "    rtol, atol = (6e-4, 2e-3) if itype == torch.float32 else (3e-3, 5e-3)\n",
    "    if itype == torch.bfloat16:\n",
    "        rtol, atol = 3e-2, 5e-2\n",
    "    rtolw, atolw = (1e-3, 1e-3)\n",
    "    # If we have z, the errors on the weights seem higher\n",
    "    rtolw = max(rtolw, rtol)\n",
    "    atolw = max(atolw, atol)\n",
    "    # set seed\n",
    "    torch.random.manual_seed(0)\n",
    "    batch_size = 2 // 2\n",
    "    dim = 768 // 8\n",
    "    dstate = 8 // 8\n",
    "    dt_rank = 48 // 8\n",
    "    is_complex = wtype == torch.complex64\n",
    "    xz = torch.randn(batch_size, 2 * dim, seqlen, device=device, dtype=itype, requires_grad=True)\n",
    "    conv1d_weight = torch.randn(dim, 1, 3, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    conv1d_bias = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    x_proj_weight = torch.randn(dt_rank + (bool(is_variable_B) + bool(is_variable_C)) * dstate\n",
    "                                * (1 if not is_complex else 2),\n",
    "                                dim, device=device, dtype=itype, requires_grad=True)\n",
    "    delta_proj_weight = torch.randn(dim, dt_rank, device=device, dtype=itype, requires_grad=True)\n",
    "    out_proj_weight = torch.randn(dim // 2, dim, device=device, dtype=itype, requires_grad=True)\n",
    "    out_proj_bias = None\n",
    "    A = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)).requires_grad_()\n",
    "    A_b = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)).requires_grad_()\n",
    "    B = (torch.randn(dim, dstate, device=device, dtype=wtype, requires_grad=True)\n",
    "         if not is_variable_B else None)\n",
    "    C = (torch.randn(dim, dstate, device=device, dtype=wtype, requires_grad=True)\n",
    "         if not is_variable_C else None)\n",
    "    D = torch.randn(dim, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    delta_bias = (0.5 * torch.rand(dim, device=device, dtype=torch.float32)).requires_grad_()\n",
    "    B_proj_bias = None\n",
    "    C_proj_bias = None\n",
    "    xz_ref = xz.detach().clone().requires_grad_()\n",
    "    conv1d_weight_ref = conv1d_weight.detach().clone().requires_grad_()\n",
    "    conv1d_bias_ref = conv1d_bias.detach().clone().requires_grad_()\n",
    "    x_proj_weight_ref = x_proj_weight.detach().clone().requires_grad_()\n",
    "    delta_proj_weight_ref = delta_proj_weight.detach().clone().requires_grad_()\n",
    "    out_proj_weight_ref = out_proj_weight.detach().clone().requires_grad_()\n",
    "    out_proj_bias_ref = (out_proj_bias.detach().clone().requires_grad_()\n",
    "                         if out_proj_bias is not None else None)\n",
    "    A_ref = A.detach().clone().requires_grad_()\n",
    "    A_b_ref = A_b.detach().clone().requires_grad_()\n",
    "    B_ref = B.detach().clone().requires_grad_() if B is not None else None\n",
    "    C_ref = C.detach().clone().requires_grad_() if C is not None else None\n",
    "    D_ref = D.detach().clone().requires_grad_()\n",
    "    delta_bias_ref = delta_bias.detach().clone().requires_grad_() if delta_bias is not None else None\n",
    "\n",
    "    # func = bimamba_inner_fn\n",
    "    # func = mamba_inner_fn\n",
    "    func = mamba_inner_ref\n",
    "\n",
    "    # gradok = gradcheck(func, (xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,out_proj_weight, out_proj_bias, A, A_b, B, C, D, delta_bias, None, None, True))\n",
    "    gradok = gradcheck(func, (xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,out_proj_weight, out_proj_bias, A, B, C, D, delta_bias, None, None, True), eps=1e-6, atol=1e-4, nondet_tol=1.)\n",
    "    print(f'* {gradok} check_gradient_numerical bimamba_inner_fn')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_bimamba_inner_fn(True, True, 128, torch.float32, torch.float32)\n",
    "# test_mamba_inner_fn(True, True, 128, torch.float32, torch.float32)\n",
    "test_bimamba_inner_fn_grad_check(True, True, 128, torch.float32, torch.float32)\n",
    "\n",
    "# input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
    "# test = gradcheck(torch.nn.functional.linear, input, eps=1e-6, atol=1e-4)\n",
    "# print(test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
