{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023, Tri Dao, Albert Gu.\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Generation benchmarking\")\n",
    "parser.add_argument(\"--model-name\", type=str, default=\"state-spaces/mamba-130m\")\n",
    "parser.add_argument(\"--prompt\", type=str, default=None)\n",
    "parser.add_argument(\"--promptlen\", type=int, default=100)\n",
    "parser.add_argument(\"--genlen\", type=int, default=100)\n",
    "parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "parser.add_argument(\"--topk\", type=int, default=1)\n",
    "parser.add_argument(\"--topp\", type=float, default=1.0)\n",
    "parser.add_argument(\"--batch\", type=int, default=1)\n",
    "\n",
    "## original\n",
    "# args = parser.parse_args() \n",
    "\n",
    "# for colab run\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 3\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model {args.model_name}\")\n",
    "is_mamba = args.model_name.startswith(\"state-spaces/mamba-\") or \"mamba\" in args.model_name\n",
    "\n",
    "if is_mamba:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/home/zhulianghui/VisionProjects/mamba/ckpts/gpt-neox-20b-tokenizer\")\n",
    "    model = MambaLMHeadModel.from_pretrained(args.model_name, device=device, dtype=dtype)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name, device_map={\"\": device}, torch_dtype=dtype)\n",
    "model.eval()\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.random.manual_seed(0)\n",
    "if args.prompt is None:\n",
    "    input_ids = torch.randint(1, 1000, (args.batch, args.promptlen), dtype=torch.long, device=\"cuda\")\n",
    "    attn_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
    "else:\n",
    "    tokens = tokenizer(args.prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokens.input_ids.to(device=device)\n",
    "    attn_mask = tokens.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + args.genlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_mamba:\n",
    "    fn = lambda: model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        cg=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        enable_timing=False,\n",
    "        temperature=args.temperature,\n",
    "        top_k=args.topk,\n",
    "        top_p=args.topp,\n",
    "    )\n",
    "else:\n",
    "    fn = lambda: model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attn_mask,\n",
    "        max_length=max_length,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=args.temperature,\n",
    "        top_k=args.topk,\n",
    "        top_p=args.topp,\n",
    "    )\n",
    "out = fn()\n",
    "if args.prompt is not None:\n",
    "    print(tokenizer.batch_decode(out.sequences.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(repeats):\n",
    "    fn()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Prompt length: {len(input_ids[0])}, generation length: {len(out.sequences[0]) - len(input_ids[0])}\")\n",
    "print(f\"{args.model_name} prompt processing + decoding time: {(time.time() - start) / repeats * 1000:.0f}ms\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
