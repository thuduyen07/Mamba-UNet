{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023, Tri Dao.\n",
    "# Implement residual + layer_norm / rms_norm.\n",
    "\n",
    "# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n",
    "# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.\n",
    "# This is faster for dimensions up to 8k, but after that it's much slower due to register spilling.\n",
    "# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import custom_fwd, custom_bwd\n",
    "\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n",
    "    dtype = x.dtype\n",
    "    if upcast:\n",
    "        weight = weight.float()\n",
    "        bias = bias.float() if bias is not None else None\n",
    "    if upcast:\n",
    "        x = x.float()\n",
    "        residual = residual.float() if residual is not None else residual\n",
    "    if residual is not None:\n",
    "        x = (x + residual).to(x.dtype)\n",
    "    out = F.layer_norm(x.to(weight.dtype), x.shape[-1:], weight=weight, bias=bias, eps=eps).to(\n",
    "        dtype\n",
    "    )\n",
    "    return out if not prenorm else (out, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n",
    "    dtype = x.dtype\n",
    "    if upcast:\n",
    "        weight = weight.float()\n",
    "        bias = bias.float() if bias is not None else None\n",
    "    if upcast:\n",
    "        x = x.float()\n",
    "        residual = residual.float() if residual is not None else residual\n",
    "    if residual is not None:\n",
    "        x = (x + residual).to(x.dtype)\n",
    "    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)\n",
    "    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)\n",
    "    out = out.to(dtype)\n",
    "    return out if not prenorm else (out, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({}, num_warps=1),\n",
    "        triton.Config({}, num_warps=2),\n",
    "        triton.Config({}, num_warps=4),\n",
    "        triton.Config({}, num_warps=8),\n",
    "        triton.Config({}, num_warps=16),\n",
    "        triton.Config({}, num_warps=32),\n",
    "    ],\n",
    "    key=[\"N\", \"HAS_RESIDUAL\", \"STORE_RESIDUAL_OUT\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n",
    ")\n",
    "# @triton.heuristics({\"HAS_BIAS\": lambda args: args[\"B\"] is not None})\n",
    "# @triton.heuristics({\"HAS_RESIDUAL\": lambda args: args[\"RESIDUAL\"] is not None})\n",
    "@triton.jit\n",
    "def _layer_norm_fwd_1pass_kernel(\n",
    "    X,  # pointer to the input\n",
    "    Y,  # pointer to the output\n",
    "    W,  # pointer to the weights\n",
    "    B,  # pointer to the biases\n",
    "    RESIDUAL,  # pointer to the residual\n",
    "    RESIDUAL_OUT,  # pointer to the residual\n",
    "    Mean,  # pointer to the mean\n",
    "    Rstd,  # pointer to the 1/std\n",
    "    stride_x_row,  # how much to increase the pointer when moving by 1 row\n",
    "    stride_y_row,\n",
    "    stride_res_row,\n",
    "    stride_res_out_row,\n",
    "    N,  # number of columns in X\n",
    "    eps,  # epsilon to avoid division by zero\n",
    "    IS_RMS_NORM: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "    HAS_RESIDUAL: tl.constexpr,\n",
    "    STORE_RESIDUAL_OUT: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr,\n",
    "):\n",
    "    # Map the program id to the row of X and Y it should compute.\n",
    "    row = tl.program_id(0)\n",
    "    X += row * stride_x_row\n",
    "    Y += row * stride_y_row\n",
    "    if HAS_RESIDUAL:\n",
    "        RESIDUAL += row * stride_res_row\n",
    "    if STORE_RESIDUAL_OUT:\n",
    "        RESIDUAL_OUT += row * stride_res_out_row\n",
    "    # Compute mean and variance\n",
    "    cols = tl.arange(0, BLOCK_N)\n",
    "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n",
    "    if HAS_RESIDUAL:\n",
    "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n",
    "        x += residual\n",
    "    if STORE_RESIDUAL_OUT:\n",
    "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n",
    "    if not IS_RMS_NORM:\n",
    "        mean = tl.sum(x, axis=0) / N\n",
    "        tl.store(Mean + row, mean)\n",
    "        xbar = tl.where(cols < N, x - mean, 0.0)\n",
    "        var = tl.sum(xbar * xbar, axis=0) / N\n",
    "    else:\n",
    "        xbar = tl.where(cols < N, x, 0.0)\n",
    "        var = tl.sum(xbar * xbar, axis=0) / N\n",
    "    rstd = 1 / tl.sqrt(var + eps)\n",
    "    tl.store(Rstd + row, rstd)\n",
    "    # Normalize and apply linear transformation\n",
    "    mask = cols < N\n",
    "    w = tl.load(W + cols, mask=mask).to(tl.float32)\n",
    "    if HAS_BIAS:\n",
    "        b = tl.load(B + cols, mask=mask).to(tl.float32)\n",
    "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n",
    "    y = x_hat * w + b if HAS_BIAS else x_hat * w\n",
    "    # Write output\n",
    "    tl.store(Y + cols, y, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _layer_norm_fwd(\n",
    "    x, weight, bias, eps, residual=None, out_dtype=None, residual_dtype=None, is_rms_norm=False\n",
    "):\n",
    "    if residual is not None:\n",
    "        residual_dtype = residual.dtype\n",
    "    M, N = x.shape\n",
    "    assert x.stride(-1) == 1\n",
    "    if residual is not None:\n",
    "        assert residual.stride(-1) == 1\n",
    "        assert residual.shape == (M, N)\n",
    "    assert weight.shape == (N,)\n",
    "    assert weight.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.stride(-1) == 1\n",
    "        assert bias.shape == (N,)\n",
    "    # allocate output\n",
    "    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)\n",
    "    assert y.stride(-1) == 1\n",
    "    if residual is not None or (residual_dtype is not None and residual_dtype != x.dtype):\n",
    "        residual_out = torch.empty(M, N, device=x.device, dtype=residual_dtype)\n",
    "        assert residual_out.stride(-1) == 1\n",
    "    else:\n",
    "        residual_out = None\n",
    "    mean = torch.empty((M,), dtype=torch.float32, device=\"cuda\") if not is_rms_norm else None\n",
    "    rstd = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n",
    "    # Less than 64KB per feature: enqueue fused kernel\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n",
    "    # heuristics for number of warps\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        _layer_norm_fwd_1pass_kernel[(M,)](\n",
    "            x,\n",
    "            y,\n",
    "            weight,\n",
    "            bias,\n",
    "            residual,\n",
    "            residual_out,\n",
    "            mean,\n",
    "            rstd,\n",
    "            x.stride(0),\n",
    "            y.stride(0),\n",
    "            residual.stride(0) if residual is not None else 0,\n",
    "            residual_out.stride(0) if residual_out is not None else 0,\n",
    "            N,\n",
    "            eps,\n",
    "            is_rms_norm,\n",
    "            BLOCK_N,\n",
    "            residual is not None,\n",
    "            residual_out is not None,\n",
    "            bias is not None,\n",
    "        )\n",
    "    # residual_out is None if residual is None and residual_dtype == input_dtype\n",
    "    return y, mean, rstd, residual_out if residual_out is not None else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({}, num_warps=1),\n",
    "        triton.Config({}, num_warps=2),\n",
    "        triton.Config({}, num_warps=4),\n",
    "        triton.Config({}, num_warps=8),\n",
    "        triton.Config({}, num_warps=16),\n",
    "        triton.Config({}, num_warps=32),\n",
    "    ],\n",
    "    key=[\"N\", \"HAS_DRESIDUAL\", \"STORE_DRESIDUAL\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n",
    ")\n",
    "# @triton.heuristics({\"HAS_BIAS\": lambda args: args[\"B\"] is not None})\n",
    "# @triton.heuristics({\"HAS_DRESIDUAL\": lambda args: args[\"DRESIDUAL\"] is not None})\n",
    "# @triton.heuristics({\"STORE_DRESIDUAL\": lambda args: args[\"DRESIDUAL_IN\"] is not None})\n",
    "@triton.heuristics({\"RECOMPUTE_OUTPUT\": lambda args: args[\"Y\"] is not None})\n",
    "@triton.jit\n",
    "def _layer_norm_bwd_kernel(\n",
    "    X,  # pointer to the input\n",
    "    W,  # pointer to the weights\n",
    "    B,  # pointer to the biases\n",
    "    Y,  # pointer to the output to be recomputed\n",
    "    DY,  # pointer to the output gradient\n",
    "    DX,  # pointer to the input gradient\n",
    "    DW,  # pointer to the partial sum of weights gradient\n",
    "    DB,  # pointer to the partial sum of biases gradient\n",
    "    DRESIDUAL,\n",
    "    DRESIDUAL_IN,\n",
    "    Mean,  # pointer to the mean\n",
    "    Rstd,  # pointer to the 1/std\n",
    "    stride_x_row,  # how much to increase the pointer when moving by 1 row\n",
    "    stride_y_row,\n",
    "    stride_dy_row,\n",
    "    stride_dx_row,\n",
    "    stride_dres_row,\n",
    "    stride_dres_in_row,\n",
    "    M,  # number of rows in X\n",
    "    N,  # number of columns in X\n",
    "    eps,  # epsilon to avoid division by zero\n",
    "    rows_per_program,\n",
    "    IS_RMS_NORM: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "    HAS_DRESIDUAL: tl.constexpr,\n",
    "    STORE_DRESIDUAL: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr,\n",
    "    RECOMPUTE_OUTPUT: tl.constexpr,\n",
    "):\n",
    "    # Map the program id to the elements of X, DX, and DY it should compute.\n",
    "    row_block_id = tl.program_id(0)\n",
    "    row_start = row_block_id * rows_per_program\n",
    "    cols = tl.arange(0, BLOCK_N)\n",
    "    mask = cols < N\n",
    "    X += row_start * stride_x_row\n",
    "    if HAS_DRESIDUAL:\n",
    "        DRESIDUAL += row_start * stride_dres_row\n",
    "    if STORE_DRESIDUAL:\n",
    "        DRESIDUAL_IN += row_start * stride_dres_in_row\n",
    "    DY += row_start * stride_dy_row\n",
    "    DX += row_start * stride_dx_row\n",
    "    if RECOMPUTE_OUTPUT:\n",
    "        Y += row_start * stride_y_row\n",
    "    w = tl.load(W + cols, mask=mask).to(tl.float32)\n",
    "    if RECOMPUTE_OUTPUT and HAS_BIAS:\n",
    "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n",
    "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n",
    "    if HAS_BIAS:\n",
    "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n",
    "    row_end = min((row_block_id + 1) * rows_per_program, M)\n",
    "    for row in range(row_start, row_end):\n",
    "        # Load data to SRAM\n",
    "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n",
    "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n",
    "        if not IS_RMS_NORM:\n",
    "            mean = tl.load(Mean + row)\n",
    "        rstd = tl.load(Rstd + row)\n",
    "        # Compute dx\n",
    "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n",
    "        xhat = tl.where(mask, xhat, 0.0)\n",
    "        if RECOMPUTE_OUTPUT:\n",
    "            y = xhat * w + b if HAS_BIAS else xhat * w\n",
    "            tl.store(Y + cols, y, mask=mask)\n",
    "        wdy = w * dy\n",
    "        dw += dy * xhat\n",
    "        if HAS_BIAS:\n",
    "            db += dy\n",
    "        if not IS_RMS_NORM:\n",
    "            c1 = tl.sum(xhat * wdy, axis=0) / N\n",
    "            c2 = tl.sum(wdy, axis=0) / N\n",
    "            dx = (wdy - (xhat * c1 + c2)) * rstd\n",
    "        else:\n",
    "            c1 = tl.sum(xhat * wdy, axis=0) / N\n",
    "            dx = (wdy - xhat * c1) * rstd\n",
    "        if HAS_DRESIDUAL:\n",
    "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n",
    "            dx += dres\n",
    "        # Write dx\n",
    "        if STORE_DRESIDUAL:\n",
    "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n",
    "        tl.store(DX + cols, dx, mask=mask)\n",
    "\n",
    "        X += stride_x_row\n",
    "        if HAS_DRESIDUAL:\n",
    "            DRESIDUAL += stride_dres_row\n",
    "        if STORE_DRESIDUAL:\n",
    "            DRESIDUAL_IN += stride_dres_in_row\n",
    "        if RECOMPUTE_OUTPUT:\n",
    "            Y += stride_y_row\n",
    "        DY += stride_dy_row\n",
    "        DX += stride_dx_row\n",
    "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n",
    "    if HAS_BIAS:\n",
    "        tl.store(DB + row_block_id * N + cols, db, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _layer_norm_bwd(\n",
    "    dy,\n",
    "    x,\n",
    "    weight,\n",
    "    bias,\n",
    "    eps,\n",
    "    mean,\n",
    "    rstd,\n",
    "    dresidual=None,\n",
    "    has_residual=False,\n",
    "    is_rms_norm=False,\n",
    "    x_dtype=None,\n",
    "    recompute_output=False,\n",
    "):\n",
    "    M, N = x.shape\n",
    "    assert x.stride(-1) == 1\n",
    "    assert dy.stride(-1) == 1\n",
    "    assert dy.shape == (M, N)\n",
    "    if dresidual is not None:\n",
    "        assert dresidual.stride(-1) == 1\n",
    "        assert dresidual.shape == (M, N)\n",
    "    assert weight.shape == (N,)\n",
    "    assert weight.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.stride(-1) == 1\n",
    "        assert bias.shape == (N,)\n",
    "    # allocate output\n",
    "    dx = (\n",
    "        torch.empty_like(x)\n",
    "        if x_dtype is None\n",
    "        else torch.empty(M, N, dtype=x_dtype, device=x.device)\n",
    "    )\n",
    "    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None\n",
    "    y = torch.empty(M, N, dtype=dy.dtype, device=dy.device) if recompute_output else None\n",
    "\n",
    "    # Less than 64KB per feature: enqueue fused kernel\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n",
    "    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count\n",
    "    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)\n",
    "    _db = (\n",
    "        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)\n",
    "        if bias is not None\n",
    "        else None\n",
    "    )\n",
    "    rows_per_program = math.ceil(M / sm_count)\n",
    "    grid = (sm_count,)\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        _layer_norm_bwd_kernel[grid](\n",
    "            x,\n",
    "            weight,\n",
    "            bias,\n",
    "            y,\n",
    "            dy,\n",
    "            dx,\n",
    "            _dw,\n",
    "            _db,\n",
    "            dresidual,\n",
    "            dresidual_in,\n",
    "            mean,\n",
    "            rstd,\n",
    "            x.stride(0),\n",
    "            0 if not recompute_output else y.stride(0),\n",
    "            dy.stride(0),\n",
    "            dx.stride(0),\n",
    "            dresidual.stride(0) if dresidual is not None else 0,\n",
    "            dresidual_in.stride(0) if dresidual_in is not None else 0,\n",
    "            M,\n",
    "            N,\n",
    "            eps,\n",
    "            rows_per_program,\n",
    "            is_rms_norm,\n",
    "            BLOCK_N,\n",
    "            dresidual is not None,\n",
    "            dresidual_in is not None,\n",
    "            bias is not None,\n",
    "        )\n",
    "    dw = _dw.sum(0).to(weight.dtype)\n",
    "    db = _db.sum(0).to(bias.dtype) if bias is not None else None\n",
    "    # Don't need to compute dresidual_in separately in this case\n",
    "    if has_residual and dx.dtype == x.dtype:\n",
    "        dresidual_in = dx\n",
    "    return (dx, dw, db, dresidual_in) if not recompute_output else (dx, dw, db, dresidual_in, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x,\n",
    "        weight,\n",
    "        bias,\n",
    "        residual=None,\n",
    "        eps=1e-6,\n",
    "        prenorm=False,\n",
    "        residual_in_fp32=False,\n",
    "        is_rms_norm=False,\n",
    "    ):\n",
    "        x_shape_og = x.shape\n",
    "        # reshape input data into 2D tensor\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        if x.stride(-1) != 1:\n",
    "            x = x.contiguous()\n",
    "        if residual is not None:\n",
    "            assert residual.shape == x_shape_og\n",
    "            residual = residual.reshape(-1, residual.shape[-1])\n",
    "            if residual.stride(-1) != 1:\n",
    "                residual = residual.contiguous()\n",
    "        weight = weight.contiguous()\n",
    "        if bias is not None:\n",
    "            bias = bias.contiguous()\n",
    "        residual_dtype = (\n",
    "            residual.dtype\n",
    "            if residual is not None\n",
    "            else (torch.float32 if residual_in_fp32 else None)\n",
    "        )\n",
    "        y, mean, rstd, residual_out = _layer_norm_fwd(\n",
    "            x, weight, bias, eps, residual, residual_dtype=residual_dtype, is_rms_norm=is_rms_norm\n",
    "        )\n",
    "        ctx.save_for_backward(residual_out, weight, bias, mean, rstd)\n",
    "        ctx.x_shape_og = x_shape_og\n",
    "        ctx.eps = eps\n",
    "        ctx.is_rms_norm = is_rms_norm\n",
    "        ctx.has_residual = residual is not None\n",
    "        ctx.prenorm = prenorm\n",
    "        ctx.x_dtype = x.dtype\n",
    "        y = y.reshape(x_shape_og)\n",
    "        return y if not prenorm else (y, residual_out.reshape(x_shape_og))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy, *args):\n",
    "        x, weight, bias, mean, rstd = ctx.saved_tensors\n",
    "        dy = dy.reshape(-1, dy.shape[-1])\n",
    "        if dy.stride(-1) != 1:\n",
    "            dy = dy.contiguous()\n",
    "        assert dy.shape == x.shape\n",
    "        if ctx.prenorm:\n",
    "            dresidual = args[0]\n",
    "            dresidual = dresidual.reshape(-1, dresidual.shape[-1])\n",
    "            if dresidual.stride(-1) != 1:\n",
    "                dresidual = dresidual.contiguous()\n",
    "            assert dresidual.shape == x.shape\n",
    "        else:\n",
    "            dresidual = None\n",
    "        dx, dw, db, dresidual_in = _layer_norm_bwd(\n",
    "            dy,\n",
    "            x,\n",
    "            weight,\n",
    "            bias,\n",
    "            ctx.eps,\n",
    "            mean,\n",
    "            rstd,\n",
    "            dresidual,\n",
    "            ctx.has_residual,\n",
    "            ctx.is_rms_norm,\n",
    "            x_dtype=ctx.x_dtype,\n",
    "        )\n",
    "        return (\n",
    "            dx.reshape(ctx.x_shape_og),\n",
    "            dw,\n",
    "            db,\n",
    "            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm_fn(\n",
    "    x,\n",
    "    weight,\n",
    "    bias,\n",
    "    residual=None,\n",
    "    eps=1e-6,\n",
    "    prenorm=False,\n",
    "    residual_in_fp32=False,\n",
    "    is_rms_norm=False,\n",
    "):\n",
    "    return LayerNormFn.apply(x, weight, bias, residual, eps, prenorm, residual_in_fp32, is_rms_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm_fn(x, weight, bias, residual=None, prenorm=False, residual_in_fp32=False, eps=1e-6):\n",
    "    return LayerNormFn.apply(x, weight, bias, residual, eps, prenorm, residual_in_fp32, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-5, device=None, dtype=None):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))\n",
    "        self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.ones_(self.weight)\n",
    "\n",
    "    def forward(self, x, residual=None, prenorm=False, residual_in_fp32=False):\n",
    "        return rms_norm_fn(\n",
    "            x,\n",
    "            self.weight,\n",
    "            self.bias,\n",
    "            residual=residual,\n",
    "            eps=self.eps,\n",
    "            prenorm=prenorm,\n",
    "            residual_in_fp32=residual_in_fp32,\n",
    "            is_rms_norm=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormLinearFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x,\n",
    "        norm_weight,\n",
    "        norm_bias,\n",
    "        linear_weight,\n",
    "        linear_bias,\n",
    "        residual=None,\n",
    "        eps=1e-6,\n",
    "        prenorm=False,\n",
    "        residual_in_fp32=False,\n",
    "        is_rms_norm=False,\n",
    "    ):\n",
    "        x_shape_og = x.shape\n",
    "        # reshape input data into 2D tensor\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        if x.stride(-1) != 1:\n",
    "            x = x.contiguous()\n",
    "        if residual is not None:\n",
    "            assert residual.shape == x_shape_og\n",
    "            residual = residual.reshape(-1, residual.shape[-1])\n",
    "            if residual.stride(-1) != 1:\n",
    "                residual = residual.contiguous()\n",
    "        norm_weight = norm_weight.contiguous()\n",
    "        if norm_bias is not None:\n",
    "            norm_bias = norm_bias.contiguous()\n",
    "        residual_dtype = (\n",
    "            residual.dtype\n",
    "            if residual is not None\n",
    "            else (torch.float32 if residual_in_fp32 else None)\n",
    "        )\n",
    "        y, mean, rstd, residual_out = _layer_norm_fwd(\n",
    "            x,\n",
    "            norm_weight,\n",
    "            norm_bias,\n",
    "            eps,\n",
    "            residual,\n",
    "            out_dtype=None if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype(),\n",
    "            residual_dtype=residual_dtype,\n",
    "            is_rms_norm=is_rms_norm,\n",
    "        )\n",
    "        y = y.reshape(x_shape_og)\n",
    "        dtype = torch.get_autocast_gpu_dtype() if torch.is_autocast_enabled() else y.dtype\n",
    "        linear_weight = linear_weight.to(dtype)\n",
    "        linear_bias = linear_bias.to(dtype) if linear_bias is not None else None\n",
    "        out = F.linear(y.to(linear_weight.dtype), linear_weight, linear_bias)\n",
    "        # We don't store y, will be recomputed in the backward pass to save memory\n",
    "        ctx.save_for_backward(residual_out, norm_weight, norm_bias, linear_weight, mean, rstd)\n",
    "        ctx.x_shape_og = x_shape_og\n",
    "        ctx.eps = eps\n",
    "        ctx.is_rms_norm = is_rms_norm\n",
    "        ctx.has_residual = residual is not None\n",
    "        ctx.prenorm = prenorm\n",
    "        ctx.x_dtype = x.dtype\n",
    "        ctx.linear_bias_is_none = linear_bias is None\n",
    "        return out if not prenorm else (out, residual_out.reshape(x_shape_og))\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, dout, *args):\n",
    "        x, norm_weight, norm_bias, linear_weight, mean, rstd = ctx.saved_tensors\n",
    "        dout = dout.reshape(-1, dout.shape[-1])\n",
    "        dy = F.linear(dout, linear_weight.t())\n",
    "        dlinear_bias = None if ctx.linear_bias_is_none else dout.sum(0)\n",
    "        if dy.stride(-1) != 1:\n",
    "            dy = dy.contiguous()\n",
    "        assert dy.shape == x.shape\n",
    "        if ctx.prenorm:\n",
    "            dresidual = args[0]\n",
    "            dresidual = dresidual.reshape(-1, dresidual.shape[-1])\n",
    "            if dresidual.stride(-1) != 1:\n",
    "                dresidual = dresidual.contiguous()\n",
    "            assert dresidual.shape == x.shape\n",
    "        else:\n",
    "            dresidual = None\n",
    "        dx, dnorm_weight, dnorm_bias, dresidual_in, y = _layer_norm_bwd(\n",
    "            dy,\n",
    "            x,\n",
    "            norm_weight,\n",
    "            norm_bias,\n",
    "            ctx.eps,\n",
    "            mean,\n",
    "            rstd,\n",
    "            dresidual,\n",
    "            ctx.has_residual,\n",
    "            ctx.is_rms_norm,\n",
    "            x_dtype=ctx.x_dtype,\n",
    "            recompute_output=True,\n",
    "        )\n",
    "        dlinear_weight = torch.einsum(\"bo,bi->oi\", dout, y)\n",
    "        return (\n",
    "            dx.reshape(ctx.x_shape_og),\n",
    "            dnorm_weight,\n",
    "            dnorm_bias,\n",
    "            dlinear_weight,\n",
    "            dlinear_bias,\n",
    "            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm_linear_fn(\n",
    "    x,\n",
    "    norm_weight,\n",
    "    norm_bias,\n",
    "    linear_weight,\n",
    "    linear_bias,\n",
    "    residual=None,\n",
    "    eps=1e-6,\n",
    "    prenorm=False,\n",
    "    residual_in_fp32=False,\n",
    "    is_rms_norm=False,\n",
    "):\n",
    "    return LayerNormLinearFn.apply(\n",
    "        x,\n",
    "        norm_weight,\n",
    "        norm_bias,\n",
    "        linear_weight,\n",
    "        linear_bias,\n",
    "        residual,\n",
    "        eps,\n",
    "        prenorm,\n",
    "        residual_in_fp32,\n",
    "        is_rms_norm,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
