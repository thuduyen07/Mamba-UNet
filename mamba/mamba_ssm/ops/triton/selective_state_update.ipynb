{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023, Tri Dao.\n",
    "\n",
    "\"\"\"We want triton==2.1.0 for this\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.heuristics({\"HAS_DT_BIAS\": lambda args: args[\"dt_bias_ptr\"] is not None})\n",
    "@triton.heuristics({\"HAS_D\": lambda args: args[\"D_ptr\"] is not None})\n",
    "@triton.heuristics({\"HAS_Z\": lambda args: args[\"z_ptr\"] is not None})\n",
    "@triton.heuristics({\"BLOCK_SIZE_DSTATE\": lambda args: triton.next_power_of_2(args[\"dstate\"])})\n",
    "@triton.jit\n",
    "def _selective_scan_update_kernel(\n",
    "    # Pointers to matrices\n",
    "    state_ptr, x_ptr, dt_ptr, dt_bias_ptr, A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr,\n",
    "    # Matrix dimensions\n",
    "    batch, dim, dstate,\n",
    "    # Strides\n",
    "    stride_state_batch, stride_state_dim, stride_state_dstate,\n",
    "    stride_x_batch, stride_x_dim,\n",
    "    stride_dt_batch, stride_dt_dim,\n",
    "    stride_dt_bias_dim,\n",
    "    stride_A_dim, stride_A_dstate,\n",
    "    stride_B_batch, stride_B_dstate,\n",
    "    stride_C_batch, stride_C_dstate,\n",
    "    stride_D_dim,\n",
    "    stride_z_batch, stride_z_dim,\n",
    "    stride_out_batch, stride_out_dim,\n",
    "    # Meta-parameters\n",
    "    DT_SOFTPLUS: tl.constexpr,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    HAS_DT_BIAS: tl.constexpr,\n",
    "    HAS_D: tl.constexpr,\n",
    "    HAS_Z: tl.constexpr,\n",
    "    BLOCK_SIZE_DSTATE: tl.constexpr,\n",
    "):\n",
    "    pid_m = tl.program_id(axis=0)\n",
    "    pid_b = tl.program_id(axis=1)\n",
    "    state_ptr += pid_b * stride_state_batch\n",
    "    x_ptr += pid_b * stride_x_batch\n",
    "    dt_ptr += pid_b * stride_dt_batch\n",
    "    B_ptr += pid_b * stride_B_batch\n",
    "    C_ptr += pid_b * stride_C_batch\n",
    "    if HAS_Z:\n",
    "        z_ptr += pid_b * stride_z_batch\n",
    "    out_ptr += pid_b * stride_out_batch\n",
    "\n",
    "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n",
    "    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate)\n",
    "    x_ptrs = x_ptr + offs_m * stride_x_dim\n",
    "    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n",
    "    if HAS_DT_BIAS:\n",
    "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n",
    "    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate)\n",
    "    B_ptrs = B_ptr + offs_n * stride_B_dstate\n",
    "    C_ptrs = C_ptr + offs_n * stride_C_dstate\n",
    "    if HAS_D:\n",
    "        D_ptrs = D_ptr + offs_m * stride_D_dim\n",
    "    if HAS_Z:\n",
    "        z_ptrs = z_ptr + offs_m * stride_z_dim\n",
    "    out_ptrs = out_ptr + offs_m * stride_out_dim\n",
    "\n",
    "    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0)\n",
    "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n",
    "    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n",
    "    if HAS_DT_BIAS:\n",
    "        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n",
    "    if DT_SOFTPLUS:\n",
    "        dt = tl.log(1.0 + tl.exp(dt))\n",
    "    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n",
    "    dA = tl.exp(A * dt[:, None])\n",
    "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\n",
    "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\n",
    "    if HAS_D:\n",
    "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n",
    "    if HAS_Z:\n",
    "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n",
    "\n",
    "    dB = B[None, :] * dt[:, None]\n",
    "    state = state * dA + dB * x[:, None]\n",
    "    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate))\n",
    "    out = tl.sum(state * C[None, :], axis=1)\n",
    "    if HAS_D:\n",
    "        out += x * D\n",
    "    if HAS_Z:\n",
    "        out *= z * tl.sigmoid(z)\n",
    "    tl.store(out_ptrs, out, mask=offs_m < dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_state_update(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "        state: (batch, dim, dstate)\n",
    "        x: (batch, dim)\n",
    "        dt: (batch, dim)\n",
    "        A: (dim, dstate)\n",
    "        B: (batch, dstate)\n",
    "        C: (batch, dstate)\n",
    "        D: (dim,)\n",
    "        z: (batch, dim)\n",
    "        dt_bias: (dim,)\n",
    "    Return:\n",
    "        out: (batch, dim)\n",
    "    \"\"\"\n",
    "    batch, dim, dstate = state.shape\n",
    "    assert x.shape == (batch, dim)\n",
    "    assert dt.shape == x.shape\n",
    "    assert A.shape == (dim, dstate)\n",
    "    assert B.shape == (batch, dstate)\n",
    "    assert C.shape == B.shape\n",
    "    if D is not None:\n",
    "        assert D.shape == (dim,)\n",
    "    if z is not None:\n",
    "        assert z.shape == x.shape\n",
    "    if dt_bias is not None:\n",
    "        assert dt_bias.shape == (dim,)\n",
    "    out = torch.empty_like(x)\n",
    "    grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch)\n",
    "    z_strides = ((z.stride(0), z.stride(1)) if z is not None else (0, 0))\n",
    "    # We don't want autotune since it will overwrite the state\n",
    "    # We instead tune by hand.\n",
    "    BLOCK_SIZE_M, num_warps = ((32, 4) if dstate <= 16\n",
    "                               else ((16, 4) if dstate <= 32 else\n",
    "                                     ((8, 4) if dstate <= 64 else\n",
    "                                      ((4, 4) if dstate <= 128 else\n",
    "                                       ((4, 8))))))\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        _selective_scan_update_kernel[grid](\n",
    "            state, x, dt, dt_bias, A, B, C, D, z, out,\n",
    "            batch, dim, dstate,\n",
    "            state.stride(0), state.stride(1), state.stride(2),\n",
    "            x.stride(0), x.stride(1),\n",
    "            dt.stride(0), dt.stride(1),\n",
    "            dt_bias.stride(0) if dt_bias is not None else 0,\n",
    "            A.stride(0), A.stride(1),\n",
    "            B.stride(0), B.stride(1),\n",
    "            C.stride(0), C.stride(1),\n",
    "            D.stride(0) if D is not None else 0,\n",
    "            z_strides[0], z_strides[1],\n",
    "            out.stride(0), out.stride(1),\n",
    "            dt_softplus,\n",
    "            BLOCK_SIZE_M,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "        state: (batch, dim, dstate)\n",
    "        x: (batch, dim)\n",
    "        dt: (batch, dim)\n",
    "        A: (dim, dstate)\n",
    "        B: (batch, dstate)\n",
    "        C: (batch, dstate)\n",
    "        D: (dim,)\n",
    "        z: (batch, dim)\n",
    "        dt_bias: (dim,)\n",
    "    Return:\n",
    "        out: (batch, dim)\n",
    "    \"\"\"\n",
    "    batch, dim, dstate = state.shape\n",
    "    assert x.shape == (batch, dim)\n",
    "    assert dt.shape == x.shape\n",
    "    assert A.shape == (dim, dstate)\n",
    "    assert B.shape == (batch, dstate)\n",
    "    assert C.shape == B.shape\n",
    "    if D is not None:\n",
    "        assert D.shape == (dim,)\n",
    "    if z is not None:\n",
    "        assert z.shape == x.shape\n",
    "    if dt_bias is not None:\n",
    "        assert dt_bias.shape == (dim,)\n",
    "        dt = dt + dt_bias\n",
    "    dt = F.softplus(dt) if dt_softplus else dt\n",
    "    dA = torch.exp(rearrange(dt, \"b d -> b d 1\") * A)  # (batch, dim, dstate)\n",
    "    dB = rearrange(dt, \"b d -> b d 1\") * rearrange(B, \"b n -> b 1 n\")  # (batch, dim, dstate)\n",
    "    state.copy_(state * dA + dB * rearrange(x, \"b d -> b d 1\"))  # (batch, dim, dstate\n",
    "    out = torch.einsum(\"bdn,bn->bd\", state.to(C.dtype), C)\n",
    "    if D is not None:\n",
    "        out += (x * D).to(out.dtype)\n",
    "    return (out if z is None else out * F.silu(z)).to(x.dtype)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
