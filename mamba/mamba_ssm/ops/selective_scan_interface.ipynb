{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023, Tri Dao, Albert Gu.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import custom_bwd, custom_fwd\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from causal_conv1d import causal_conv1d_fn\n",
    "import causal_conv1d_cuda\n",
    "import selective_scan_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveScanFn(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
    "                return_last_state=False):\n",
    "        if u.stride(-1) != 1:\n",
    "            u = u.contiguous()\n",
    "        if delta.stride(-1) != 1:\n",
    "            delta = delta.contiguous()\n",
    "        if D is not None:\n",
    "            D = D.contiguous()\n",
    "        if B.stride(-1) != 1:\n",
    "            B = B.contiguous()\n",
    "        if C.stride(-1) != 1:\n",
    "            C = C.contiguous()\n",
    "        if z is not None and z.stride(-1) != 1:\n",
    "            z = z.contiguous()\n",
    "        if B.dim() == 3:\n",
    "            B = rearrange(B, \"b dstate l -> b 1 dstate l\")\n",
    "            ctx.squeeze_B = True\n",
    "        if C.dim() == 3:\n",
    "            C = rearrange(C, \"b dstate l -> b 1 dstate l\")\n",
    "            ctx.squeeze_C = True\n",
    "        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus)\n",
    "        ctx.delta_softplus = delta_softplus\n",
    "        ctx.has_z = z is not None\n",
    "        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)\n",
    "        if not ctx.has_z:\n",
    "            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)\n",
    "            return out if not return_last_state else (out, last_state)\n",
    "        else:\n",
    "            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)\n",
    "            out_z = rest[0]\n",
    "            return out_z if not return_last_state else (out_z, last_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dout, *args):\n",
    "        if not ctx.has_z:\n",
    "            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors\n",
    "            z = None\n",
    "            out = None\n",
    "        else:\n",
    "            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors\n",
    "        if dout.stride(-1) != 1:\n",
    "            dout = dout.contiguous()\n",
    "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
    "        # backward of selective_scan_cuda with the backward of chunk).\n",
    "        # Here we just pass in None and dz will be allocated in the C++ code.\n",
    "        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(\n",
    "            u, delta, A, B, C, D, z, delta_bias, dout, x, out, None, ctx.delta_softplus,\n",
    "            False  # option to recompute out_z, not used here\n",
    "        )\n",
    "        dz = rest[0] if ctx.has_z else None\n",
    "        dB = dB.squeeze(1) if getattr(ctx, \"squeeze_B\", False) else dB\n",
    "        dC = dC.squeeze(1) if getattr(ctx, \"squeeze_C\", False) else dC\n",
    "        return (du, ddelta, dA, dB, dC,\n",
    "                dD if D is not None else None,\n",
    "                dz,\n",
    "                ddelta_bias if delta_bias is not None else None,\n",
    "                None,\n",
    "                None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_scan_fn(u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
    "                     return_last_state=False):\n",
    "    \"\"\"if return_last_state is True, returns (out, last_state)\n",
    "    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is\n",
    "    not considered in the backward pass.\n",
    "    \"\"\"\n",
    "    return SelectiveScanFn.apply(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_scan_ref(u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
    "                      return_last_state=False):\n",
    "    \"\"\"\n",
    "    u: r(B D L)\n",
    "    delta: r(B D L)\n",
    "    A: c(D N) or r(D N)\n",
    "    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
    "    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
    "    D: r(D)\n",
    "    z: r(B D L)\n",
    "    delta_bias: r(D), fp32\n",
    "\n",
    "    out: r(B D L)\n",
    "    last_state (optional): r(B D dstate) or c(B D dstate)\n",
    "    \"\"\"\n",
    "    dtype_in = u.dtype\n",
    "    u = u.float()\n",
    "    delta = delta.float()\n",
    "    if delta_bias is not None:\n",
    "        delta = delta + delta_bias[..., None].float()\n",
    "    if delta_softplus:\n",
    "        delta = F.softplus(delta)\n",
    "    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
    "    is_variable_B = B.dim() >= 3\n",
    "    is_variable_C = C.dim() >= 3\n",
    "    if A.is_complex():\n",
    "        if is_variable_B:\n",
    "            B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
    "        if is_variable_C:\n",
    "            C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
    "    else:\n",
    "        B = B.float()\n",
    "        C = C.float()\n",
    "    x = A.new_zeros((batch, dim, dstate))\n",
    "    ys = []\n",
    "    deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "    if not is_variable_B:\n",
    "        deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
    "    else:\n",
    "        if B.dim() == 3:\n",
    "            deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
    "        else:\n",
    "            B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
    "            deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
    "    if is_variable_C and C.dim() == 4:\n",
    "        C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
    "    last_state = None\n",
    "    for i in range(u.shape[2]):\n",
    "        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
    "        if not is_variable_C:\n",
    "            y = torch.einsum('bdn,dn->bd', x, C)\n",
    "        else:\n",
    "            if C.dim() == 3:\n",
    "                y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
    "            else:\n",
    "                y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
    "        if i == u.shape[2] - 1:\n",
    "            last_state = x\n",
    "        if y.is_complex():\n",
    "            y = y.real * 2\n",
    "        ys.append(y)\n",
    "    y = torch.stack(ys, dim=2) # (batch dim L)\n",
    "    out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
    "    if z is not None:\n",
    "        out = out * F.silu(z)\n",
    "    out = out.to(dtype=dtype_in)\n",
    "    return out if not return_last_state else (out, last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaInnerFnNoOutProj(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "                C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1):\n",
    "        \"\"\"\n",
    "             xz: (batch, dim, seqlen)\n",
    "        \"\"\"\n",
    "        assert checkpoint_lvl in [0, 1]\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        if torch.is_autocast_enabled():\n",
    "            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "        if xz.stride(-1) != 1:\n",
    "            xz = xz.contiguous()\n",
    "        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n",
    "        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, True)\n",
    "        # We're being very careful here about the layout, to avoid extra transposes.\n",
    "        # We want delta to have d as the slowest moving dimension\n",
    "        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "        x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l = L)\n",
    "        ctx.is_variable_B = B is None\n",
    "        ctx.is_variable_C = C is None\n",
    "        ctx.B_proj_bias_is_None = B_proj_bias is None\n",
    "        ctx.C_proj_bias_is_None = C_proj_bias is None\n",
    "        if B is None:  # variable B\n",
    "            B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl dstate)\n",
    "            if B_proj_bias is not None:\n",
    "                B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "            if not A.is_complex():\n",
    "                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if B.stride(-1) != 1:\n",
    "                B = B.contiguous()\n",
    "        if C is None:  # variable C\n",
    "            C = x_dbl[:, -d_state:]  # (bl dstate)\n",
    "            if C_proj_bias is not None:\n",
    "                C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "            if not A.is_complex():\n",
    "                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if C.stride(-1) != 1:\n",
    "                C = C.contiguous()\n",
    "        if D is not None:\n",
    "            D = D.contiguous()\n",
    "        out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n",
    "        )\n",
    "        ctx.delta_softplus = delta_softplus\n",
    "        ctx.checkpoint_lvl = checkpoint_lvl\n",
    "        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n",
    "            conv1d_out, delta = None, None\n",
    "        ctx.save_for_backward(xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight,\n",
    "                              delta_proj_weight, conv1d_out, delta,\n",
    "                              A, B, C, D, delta_bias, scan_intermediates, out)\n",
    "        # return rearrange(out_z, \"b d l -> b l d\")\n",
    "        return out_z\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, dout):\n",
    "        # dout: (batch, seqlen, dim)\n",
    "        (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, \n",
    "         conv1d_out, delta, A, B, C, D, delta_bias, scan_intermediates, out) = ctx.saved_tensors\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        if dout.stride(-1) != 1:\n",
    "            dout = dout.contiguous()\n",
    "        if ctx.checkpoint_lvl == 1:\n",
    "            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, True)\n",
    "            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),\n",
    "                              \"d (b l) -> b d l\", l = L)\n",
    "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
    "        # backward of selective_scan_cuda with the backward of chunk).\n",
    "        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n",
    "        dx, dz = dxz.chunk(2, dim=1)\n",
    "        # dout_y = rearrange(dout, \"b l d -> b d l\") # because no arrange at end of forward, so dout shape is b d l\n",
    "        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, dout, scan_intermediates, out, dz,\n",
    "            ctx.delta_softplus,\n",
    "            True  # option to recompute out_z\n",
    "        )\n",
    "        dD = dD if D is not None else None\n",
    "        dx_dbl = torch.empty_like(x_dbl)\n",
    "        dB_proj_bias = None\n",
    "        if ctx.is_variable_B:\n",
    "            if not A.is_complex():\n",
    "                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n",
    "            dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)\n",
    "            dB = None\n",
    "        dC_proj_bias = None\n",
    "        if ctx.is_variable_C:\n",
    "            if not A.is_complex():\n",
    "                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n",
    "            dx_dbl[:, -d_state:] = dC  # (bl d)\n",
    "            dC = None\n",
    "        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n",
    "        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n",
    "        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n",
    "        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n",
    "        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n",
    "        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
    "        # backward of conv1d with the backward of chunk).\n",
    "        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "            x, conv1d_weight, conv1d_bias, dconv1d_out, dx, True\n",
    "        )\n",
    "        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n",
    "        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n",
    "        return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,\n",
    "                dA, dB, dC, dD,\n",
    "                ddelta_bias if delta_bias is not None else None,\n",
    "                dB_proj_bias, dC_proj_bias, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaInnerFn(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                out_proj_weight, out_proj_bias,\n",
    "                A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "                C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1):\n",
    "        \"\"\"\n",
    "             xz: (batch, dim, seqlen)\n",
    "        \"\"\"\n",
    "        assert checkpoint_lvl in [0, 1]\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        if torch.is_autocast_enabled():\n",
    "            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            out_proj_bias = (out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "                             if out_proj_bias is not None else None)\n",
    "        if xz.stride(-1) != 1:\n",
    "            xz = xz.contiguous()\n",
    "        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n",
    "        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, True)\n",
    "        # We're being very careful here about the layout, to avoid extra transposes.\n",
    "        # We want delta to have d as the slowest moving dimension\n",
    "        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "        x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l = L)\n",
    "        ctx.is_variable_B = B is None\n",
    "        ctx.is_variable_C = C is None\n",
    "        ctx.B_proj_bias_is_None = B_proj_bias is None\n",
    "        ctx.C_proj_bias_is_None = C_proj_bias is None\n",
    "        if B is None:  # variable B\n",
    "            B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl dstate)\n",
    "            if B_proj_bias is not None:\n",
    "                B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "            if not A.is_complex():\n",
    "                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if B.stride(-1) != 1:\n",
    "                B = B.contiguous()\n",
    "        if C is None:  # variable C\n",
    "            C = x_dbl[:, -d_state:]  # (bl dstate)\n",
    "            if C_proj_bias is not None:\n",
    "                C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "            if not A.is_complex():\n",
    "                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if C.stride(-1) != 1:\n",
    "                C = C.contiguous()\n",
    "        if D is not None:\n",
    "            D = D.contiguous()\n",
    "        out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n",
    "        )\n",
    "        ctx.delta_softplus = delta_softplus\n",
    "        ctx.out_proj_bias_is_None = out_proj_bias is None\n",
    "        ctx.checkpoint_lvl = checkpoint_lvl\n",
    "        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n",
    "            conv1d_out, delta = None, None\n",
    "        ctx.save_for_backward(xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight,\n",
    "                              delta_proj_weight, out_proj_weight, conv1d_out, delta,\n",
    "                              A, B, C, D, delta_bias, scan_intermediates, out)\n",
    "        return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, dout):\n",
    "        # dout: (batch, seqlen, dim)\n",
    "        (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, out_proj_weight,\n",
    "         conv1d_out, delta, A, B, C, D, delta_bias, scan_intermediates, out) = ctx.saved_tensors\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        if dout.stride(-1) != 1:\n",
    "            dout = dout.contiguous()\n",
    "        if ctx.checkpoint_lvl == 1:\n",
    "            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, True)\n",
    "            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),\n",
    "                              \"d (b l) -> b d l\", l = L)\n",
    "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
    "        # backward of selective_scan_cuda with the backward of chunk).\n",
    "        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n",
    "        dx, dz = dxz.chunk(2, dim=1)\n",
    "        dout = rearrange(dout, \"b l e -> e (b l)\")\n",
    "        dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n",
    "        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, dout_y, scan_intermediates, out, dz,\n",
    "            ctx.delta_softplus,\n",
    "            True  # option to recompute out_z\n",
    "        )\n",
    "        dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n",
    "        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n",
    "        dD = dD if D is not None else None\n",
    "        dx_dbl = torch.empty_like(x_dbl)\n",
    "        dB_proj_bias = None\n",
    "        if ctx.is_variable_B:\n",
    "            if not A.is_complex():\n",
    "                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n",
    "            dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)\n",
    "            dB = None\n",
    "        dC_proj_bias = None\n",
    "        if ctx.is_variable_C:\n",
    "            if not A.is_complex():\n",
    "                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n",
    "            dx_dbl[:, -d_state:] = dC  # (bl d)\n",
    "            dC = None\n",
    "        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n",
    "        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n",
    "        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n",
    "        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n",
    "        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n",
    "        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
    "        # backward of conv1d with the backward of chunk).\n",
    "        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "            x, conv1d_weight, conv1d_bias, dconv1d_out, dx, True\n",
    "        )\n",
    "        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n",
    "        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n",
    "        return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,\n",
    "                dout_proj_weight, dout_proj_bias,\n",
    "                dA, dB, dC, dD,\n",
    "                ddelta_bias if delta_bias is not None else None,\n",
    "                dB_proj_bias, dC_proj_bias, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiMambaInnerFn(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                out_proj_weight, out_proj_bias,\n",
    "                A, A_b, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "                C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1):\n",
    "        \"\"\"\n",
    "             xz: (batch, dim, seqlen)\n",
    "        \"\"\"\n",
    "        assert checkpoint_lvl in [0, 1]\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        if torch.is_autocast_enabled():\n",
    "            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "            out_proj_bias = (out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "                             if out_proj_bias is not None else None)\n",
    "        if xz.stride(-1) != 1:\n",
    "            xz = xz.contiguous()\n",
    "        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n",
    "        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, True)\n",
    "        # We're being very careful here about the layout, to avoid extra transposes.\n",
    "        # We want delta to have d as the slowest moving dimension\n",
    "        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "        x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l = L)\n",
    "        ctx.is_variable_B = B is None\n",
    "        ctx.is_variable_C = C is None\n",
    "        ctx.B_proj_bias_is_None = B_proj_bias is None\n",
    "        ctx.C_proj_bias_is_None = C_proj_bias is None\n",
    "        if B is None:  # variable B\n",
    "            B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl dstate)\n",
    "            if B_proj_bias is not None:\n",
    "                B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "            if not A.is_complex():\n",
    "                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if B.stride(-1) != 1:\n",
    "                B = B.contiguous()\n",
    "        if C is None:  # variable C\n",
    "            C = x_dbl[:, -d_state:]  # (bl dstate)\n",
    "            if C_proj_bias is not None:\n",
    "                C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "            if not A.is_complex():\n",
    "                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if C.stride(-1) != 1:\n",
    "                C = C.contiguous()\n",
    "        if D is not None:\n",
    "            D = D.contiguous()\n",
    "        out_f, scan_intermediates_f, out_z_f = selective_scan_cuda.fwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n",
    "        )\n",
    "        assert not A_b.is_complex(), \"A should not be complex!!\"\n",
    "        out_b, scan_intermediates_b, out_z_b = selective_scan_cuda.fwd(\n",
    "            conv1d_out.flip([-1]), delta.flip([-1]), A_b, B.flip([-1]), C.flip([-1]), D, z.flip([-1]), delta_bias, delta_softplus,\n",
    "        )\n",
    "\n",
    "        out_z = out_z_f + out_z_b.flip([-1])\n",
    "\n",
    "        ctx.delta_softplus = delta_softplus\n",
    "        ctx.out_proj_bias_is_None = out_proj_bias is None\n",
    "        ctx.checkpoint_lvl = checkpoint_lvl\n",
    "        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n",
    "            conv1d_out, delta = None, None\n",
    "        ctx.save_for_backward(xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight,\n",
    "                              delta_proj_weight, out_proj_weight, conv1d_out, delta,\n",
    "                              A, A_b, B, C, D, delta_bias, scan_intermediates_f, scan_intermediates_b, out_f, out_b)\n",
    "        return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, dout):\n",
    "        # dout: (batch, seqlen, dim)\n",
    "        (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, out_proj_weight,\n",
    "         conv1d_out, delta, A, A_b, B, C, D, delta_bias, scan_intermediates_f, scan_intermediates_b, out_f, out_b) = ctx.saved_tensors\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        if dout.stride(-1) != 1:\n",
    "            dout = dout.contiguous()\n",
    "        if ctx.checkpoint_lvl == 1:\n",
    "            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, True)\n",
    "            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),\n",
    "                              \"d (b l) -> b d l\", l = L)\n",
    "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
    "        # backward of selective_scan_cuda with the backward of chunk).\n",
    "        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n",
    "        dx, dz = dxz.chunk(2, dim=1)\n",
    "        dout = rearrange(dout, \"b l e -> e (b l)\")\n",
    "        dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n",
    "        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z_f = selective_scan_cuda.bwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, dout_y, scan_intermediates_f, out_f, dz,\n",
    "            ctx.delta_softplus,\n",
    "            True  # option to recompute out_z\n",
    "        )\n",
    "        # flip one\n",
    "        dz_b = torch.empty_like(dz)\n",
    "        dconv1d_out_f_b, ddelta_f_b, dA_b, dB_f_b, dC_f_b, dD_b, ddelta_bias_b, dz_b, out_z_b = selective_scan_cuda.bwd(\n",
    "            conv1d_out.flip([-1]), delta.flip([-1]), A_b, B.flip([-1]), C.flip([-1]), D, z.flip([-1]), delta_bias, dout_y.flip([-1]), scan_intermediates_b, out_b, dz_b,\n",
    "            ctx.delta_softplus,\n",
    "            True  # option to recompute out_z\n",
    "        )\n",
    "\n",
    "        dconv1d_out = dconv1d_out + dconv1d_out_f_b.flip([-1])\n",
    "        ddelta = ddelta + ddelta_f_b.flip([-1])\n",
    "        dB = dB + dB_f_b.flip([-1])\n",
    "        dC = dC + dC_f_b.flip([-1])\n",
    "        dD = dD + dD_b\n",
    "        ddelta_bias = ddelta_bias + ddelta_bias_b\n",
    "        dz = dz + dz_b.flip([-1])\n",
    "        out_z = out_z_f + out_z_b.flip([-1])\n",
    "        \n",
    "        dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n",
    "        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n",
    "        dD = dD if D is not None else None\n",
    "        dx_dbl = torch.empty_like(x_dbl)\n",
    "        dB_proj_bias = None\n",
    "        if ctx.is_variable_B:\n",
    "            if not A.is_complex():\n",
    "                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n",
    "            dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)\n",
    "            dB = None\n",
    "        dC_proj_bias = None\n",
    "        if ctx.is_variable_C:\n",
    "            if not A.is_complex():\n",
    "                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n",
    "            dx_dbl[:, -d_state:] = dC  # (bl d)\n",
    "            dC = None\n",
    "        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n",
    "        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n",
    "        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n",
    "        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n",
    "        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n",
    "        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
    "        # backward of conv1d with the backward of chunk).\n",
    "        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "            x, conv1d_weight, conv1d_bias, dconv1d_out, dx, True\n",
    "        )\n",
    "        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n",
    "        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n",
    "        return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,\n",
    "                dout_proj_weight, dout_proj_bias,\n",
    "                dA, dA_b, dB, dC, dD,\n",
    "                ddelta_bias if delta_bias is not None else None,\n",
    "                dB_proj_bias, dC_proj_bias, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mamba_inner_fn(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    return MambaInnerFn.apply(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                              out_proj_weight, out_proj_bias,\n",
    "                              A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bimamba_inner_fn(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, A_b, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    return BiMambaInnerFn.apply(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                              out_proj_weight, out_proj_bias,\n",
    "                              A, A_b, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mamba_inner_fn_no_out_proj(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    return MambaInnerFnNoOutProj.apply(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                              A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mamba_inner_ref(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    L = xz.shape[-1]\n",
    "    delta_rank = delta_proj_weight.shape[1]\n",
    "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "    x, z = xz.chunk(2, dim=1)\n",
    "    x = causal_conv1d_fn(x, rearrange(conv1d_weight, \"d 1 w -> d w\"), conv1d_bias, \"silu\")\n",
    "    # We're being very careful here about the layout, to avoid extra transposes.\n",
    "    # We want delta to have d as the slowest moving dimension\n",
    "    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "    x_dbl = F.linear(rearrange(x, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()\n",
    "    delta = rearrange(delta, \"d (b l) -> b d l\", l=L)\n",
    "    if B is None:  # variable B\n",
    "        B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl d)\n",
    "        if B_proj_bias is not None:\n",
    "            B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "        if not A.is_complex():\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            B = rearrange(B, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    if C is None:  # variable B\n",
    "        C = x_dbl[:, -d_state:]  # (bl d)\n",
    "        if C_proj_bias is not None:\n",
    "            C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "        if not A.is_complex():\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            C = rearrange(C, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    y = selective_scan_fn(x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True)\n",
    "    return F.linear(rearrange(y, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bimamba_inner_ref(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, A_b, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    L = xz.shape[-1]\n",
    "    delta_rank = delta_proj_weight.shape[1]\n",
    "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "    x, z = xz.chunk(2, dim=1)\n",
    "    x = causal_conv1d_fn(x, rearrange(conv1d_weight, \"d 1 w -> d w\"), conv1d_bias, \"silu\")\n",
    "    # We're being very careful here about the layout, to avoid extra transposes.\n",
    "    # We want delta to have d as the slowest moving dimension\n",
    "    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "    x_dbl = F.linear(rearrange(x, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()\n",
    "    delta = rearrange(delta, \"d (b l) -> b d l\", l=L)\n",
    "    if B is None:  # variable B\n",
    "        B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl d)\n",
    "        if B_proj_bias is not None:\n",
    "            B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "        if not A.is_complex():\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            B = rearrange(B, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    if C is None:  # variable B\n",
    "        C = x_dbl[:, -d_state:]  # (bl d)\n",
    "        if C_proj_bias is not None:\n",
    "            C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "        if not A.is_complex():\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            C = rearrange(C, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    y = selective_scan_fn(x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True)\n",
    "    y_b = selective_scan_fn(x.flip([-1]), delta.flip([-1]), A_b, B.flip([-1]), C.flip([-1]), D, z.flip([-1]), delta_bias, delta_softplus=True)\n",
    "    y = y + y_b.flip([-1])\n",
    "    return F.linear(rearrange(y, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
